{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import cv2\n",
        "import numpy as np\n",
        "from numpy.linalg import inv\n",
        "\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "from torch import Tensor\n",
        "\n",
        "from depthNet_model import depthNet\n",
        "from visualize import np2Depth\n",
        "\n",
        "# Model initialization\n",
        "depthnet = depthNet()\n",
        "model_data = torch.load('/content/opensource_model.pth.tar')\n",
        "depthnet.load_state_dict(model_data['state_dict'])\n",
        "depthnet = depthnet.cuda()\n",
        "cudnn.benchmark = True\n",
        "depthnet.eval()\n",
        "\n",
        "# Prepare pixel coordinates for cost volume calculation\n",
        "pixel_coordinate = np.indices([320, 256]).astype(np.float32)\n",
        "pixel_coordinate = np.concatenate((pixel_coordinate, np.ones([1, 320, 256])), axis=0)\n",
        "pixel_coordinate = np.reshape(pixel_coordinate, [3, -1])\n",
        "\n",
        "#Details to provide\n",
        "left_image = cv2.imread(\"/content/stcg.us.0003_f8f1f2c0b57a4d9981ef602f99030def_cam-1_change-1.png\")\n",
        "right_image = cv2.imread(\"/content/stcg.us.0003_f8f1f2c0b57a4d9981ef602f99030def_cam-1_change-10001.png\")\n",
        "left_pose = np.array([\n",
        "    [0.07543147, 0.61393189, -0.78574661, 1.3405],\n",
        "    [0.9970987, -0.03837025, 0.06574118, 0.6266],\n",
        "    [0.01021131, -0.78842588, -0.61504501, 1.6575],\n",
        "    [0, 0, 0, 1]\n",
        "])\n",
        "right_pose = np.array([\n",
        "    [6.40527011e-02, 6.40832173e-01, -7.65004168e-01, 1.3160],\n",
        "    [9.97946496e-01, -4.09736058e-02, 4.92336713e-02, 0.6254],\n",
        "    [2.05541383e-04, -7.66586779e-01, -6.42140692e-01, 1.6196],\n",
        "    [0, 0, 0, 1]\n",
        "])\n",
        "camera_k = np.array([\n",
        "    [525.0, 0, 319.5],\n",
        "    [0, 525.0, 239.5],\n",
        "    [0, 0, 1]\n",
        "])\n",
        "\n",
        "# Test the epipolar line\n",
        "left2right = np.dot(inv(right_pose), left_pose)\n",
        "test_point = np.array([left_image.shape[1] / 2, left_image.shape[0] / 2, 1])\n",
        "far_point = np.dot(inv(camera_k), test_point) * 50.0\n",
        "far_point = np.append(far_point, 1)\n",
        "far_point = np.dot(left2right, far_point)\n",
        "far_pixel = np.dot(camera_k, far_point[0:3])\n",
        "far_pixel = (far_pixel / far_pixel[2])[0:2]\n",
        "near_point = np.dot(inv(camera_k), test_point) * 0.1\n",
        "near_point = np.append(near_point, 1)\n",
        "near_point = np.dot(left2right, near_point)\n",
        "near_pixel = np.dot(camera_k, near_point[0:3])\n",
        "near_pixel = (near_pixel / near_pixel[2])[0:2]\n",
        "cv2.line(right_image, (int(far_pixel[0] + 0.5), int(far_pixel[1] + 0.5)),\n",
        "         (int(near_pixel[0] + 0.5), int(near_pixel[1] + 0.5)), [0, 0, 255], 4)\n",
        "test_point = (int(test_point[0]), int(test_point[1]))\n",
        "cv2.circle(left_image, test_point, 4, [0, 0, 255], -1)\n",
        "\n",
        "# Scale images to 320x256\n",
        "original_width = left_image.shape[1]\n",
        "original_height = left_image.shape[0]\n",
        "factor_x = 320.0 / original_width\n",
        "factor_y = 256.0 / original_height\n",
        "left_image = cv2.resize(left_image, (320, 256))\n",
        "right_image = cv2.resize(right_image, (320, 256))\n",
        "camera_k[0, :] *= factor_x\n",
        "camera_k[1, :] *= factor_y\n",
        "\n",
        "# Convert images to PyTorch format\n",
        "torch_left_image = np.moveaxis(left_image, -1, 0)[np.newaxis].astype(np.float32)\n",
        "torch_left_image = (torch_left_image - 81.0) / 35.0\n",
        "torch_right_image = np.moveaxis(right_image, -1, 0)[np.newaxis].astype(np.float32)\n",
        "torch_right_image = (torch_right_image - 81.0) / 35.0\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "left_image_cuda = torch.tensor(torch_left_image, device='cuda')\n",
        "right_image_cuda = torch.tensor(torch_right_image, device='cuda')\n",
        "\n",
        "# Prepare transformation matrices\n",
        "left_in_right_T = left2right[0:3, 3]\n",
        "left_in_right_R = left2right[0:3, 0:3]\n",
        "K = camera_k\n",
        "K_inverse = inv(K)\n",
        "KRK_i = K.dot(left_in_right_R.dot(K_inverse))\n",
        "KRKiUV = KRK_i.dot(pixel_coordinate)\n",
        "KT = K.dot(left_in_right_T).reshape(1, 3, 1).astype(np.float32)\n",
        "KRKiUV = KRKiUV.reshape(1, 3, 320, 256).astype(np.float32)\n",
        "\n",
        "KRKiUV_cuda_T = torch.tensor(KRKiUV, dtype=torch.float32, device='cuda')\n",
        "KT_cuda_T = torch.tensor(KT, dtype=torch.float32, device='cuda')\n",
        "\n",
        "# Run inference\n",
        "with torch.no_grad():\n",
        "    predict_depths = depthnet(left_image_cuda, right_image_cuda, KRKiUV_cuda_T, KT_cuda_T)\n",
        "\n",
        "# Visualize the results\n",
        "idepth = np.squeeze(predict_depths[0].cpu().numpy())\n",
        "np_depth = np2Depth(idepth, np.zeros(idepth.shape, dtype=bool))\n",
        "result_image = np.concatenate((left_image, right_image, np_depth), axis=1)\n",
        "cv2.imshow(\"result\", result_image)\n",
        "cv2.waitKey(0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "ojcCI7Ef-UOw",
        "outputId": "4f86264b-9fec-4d4a-e315-17d150485116"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/depthNet_model.py:120: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
            "  init.constant(m.bias, 0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/opensource_model.pth.tar'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d7e7729c35f3>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Model initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdepthnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdepthNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodel_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/opensource_model.pth.tar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mdepthnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mdepthnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdepthnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    999\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/opensource_model.pth.tar'"
          ]
        }
      ]
    }
  ]
}